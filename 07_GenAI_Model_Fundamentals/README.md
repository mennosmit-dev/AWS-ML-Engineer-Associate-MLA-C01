**Course:** https://www.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/  
**Section:** 07 — Generative AI Model Fundamentals

# 07 — Generative AI Model Fundamentals

## Professional Focus
Build a strong conceptual base for transformers and LLMs so design decisions are grounded, not buzzword-driven.

## What This Enables Me To Do
- Understand transformer mechanics (attention, tokens, embeddings)\n- Know what fine-tuning vs transfer learning means for LLMs\n- Practice notebooks/labs that connect theory to implementation\n- Understand how AWS positions foundation models and JumpStart

## AWS Services / Concepts Covered
- Transformers, attention, tokens/embeddings/temperature\n- Fine-tuning vs transfer learning\n- SageMaker notebooks labs\n- JumpStart + Hugging Face workflows

## Interview-Ready Takeaways
- I can explain LLM fundamentals clearly to technical and non-technical stakeholders\n- I understand practical controls that affect outputs (temperature, tokens, etc.)

## Lecture Map (for navigation)
- Transformer architecture + self-attention\n- GPT mechanics + key terms\n- Labs: tokenization, attention, GPT in SageMaker\n- JumpStart + Hugging Face lab\n- Quiz

## Evidence I’ll add (optional)
- notes/
- lab-notes/
- diagrams/
